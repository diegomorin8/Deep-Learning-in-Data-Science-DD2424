function [grad_W, grad_b] = BackwardBN(X, Y, P, W, h, s, s_norm, mu_, v, lambda)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% where
%  - X = each column of X corresponds to an image and it has size dxn.
%  - Y = each column of Y (Kxn) is the one-hot ground truth label for the 
%         corresponding column of X.
%  - P = each column of P contains the probability for each label for the image
%         in the corresponding column of X. P has size Kxn.
%  - grad_W =  is the gradient matrix of the cost J relative to W and has size
%               Kxd.
%  - grad_b = is the gradient vector of the cost J relative to b and has size
%              Kx1.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % This fucntion has been written following the indications included in the
    % last slides of lecture 3. 

    % Initialize gradients
    for i = 1:size(W,2)
        grad_W{i} = zeros(size(W{i}));
        grad_b{i} = zeros(size(W{i},1),1);
    end
    
    G = cell(1,size(W,2));
    % We need to iterate for each image, as we need to compute the gradient
    % for each image. 
    for i = 1:size(X, 2)

        % We calculate for every image the equation g.
        G{size(W,2)}(i,:) = - Y(:,i)'/(Y(:,i)'*P(:,i)) * (diag(P(:,i)) - P(:, i)*P(:, i)');
            
        % Propagate gradients
        G{size(W,2)-1}(i,:) = G{size(W,2)}(i,:)*W{size(W,2)};

        s1 = s_norm{size(W,2)-1}(:,i)';
        s1 = diag( s1 > 0 );

        % Gradient propagation
        G{size(W,2)-1}(i,:) = G{size(W,2)-1}(i,:)*s1;
    end
    % Gradient with respect to W2 and b2
    grad_b{size(W,2)} = sum(G{size(W,2)},1)';
    grad_W{size(W,2)} = G{size(W,2)}'*h{size(W,2)}';
    
    for j = (size(W,2)-1):-1:1
        G{j} = BatchNormBackProp( G{j}, s{j}, mu_{j}, v{j} );
        for i = 1:size(X, 2)
            % Gradient with respect to W2 and b2
            grad_b{j} = grad_b{j} + g{i}';
            grad_W{j} = grad_W{j} + g{i}'*h{j}(:, i)';
            
            if j > 1
                % Propagate gradients
                g{i} = g{i}*W{j};

                s1 = s_norm{j-1}(:,i)';
                s1 = diag( s1 > 0 );

                % Gradient propagation
                g{i} = g{i}*s1;
            end
        end
    end
    % We have to divide the summatory between the batch size
    % Size of the batch
    B = size(X,2);

    for i = 1:size(W,2)
        % The last step is to add the regularization term
        grad_W{i} = grad_W{i}/B + 2*lambda*W{i};
        grad_b{i} = grad_b{i}/B;
    end

