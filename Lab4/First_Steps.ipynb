{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course: DD2424 - Assignment 4\n",
    "\n",
    "The main objective of this assignment is to train a RNN to synthesize English text character by character. The training will be done using the text from the book _Harry potter and The Goblet of Fire_ by J.K.Rowling. AdaGrad was used as the main optimizing function. The main steps that were perfomed during the implementation of the current Assignment were: \n",
    "1. __Preparing Data__: Read in the training data, determine the number of unique characters in the text and set up mapping functions - one mapping each character to a unique index and another mapping each index to a character.\n",
    "2. __Back-propagation__: The forward and the backward pass of the backpropagation algorithm for a vanilla RNN to efficiently compute the gradients.\n",
    "3. __AdaGrad updating__ the RNN’s parameters.\n",
    "4. __Synthesizing__ text from the RNN: Given a learnt set of parameters for the RNN, a default initial hidden state h0 and an initial input vector, x0, from which to bootstrap from then you will write a function to generate a sequence of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read the data\n",
    "\n",
    "The data is obtained from a Text file that include the whole _Harry Potter and the Goblet of Fire_ book. Then, the first step is to read it from the text file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the book\n",
    "book_fname = 'data/goblet.txt';\n",
    "book_data = open(book_fname,'r').read();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '6', '7', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}', '\\x80', '\\xa2', '\\xbc', '\\xc3', '\\xe2'],)\n"
     ]
    }
   ],
   "source": [
    "book_chars = ''.join(set(book_data))\n",
    "book_chars = list(sorted(book_chars))\n",
    "# Show all the characters\n",
    "print(book_chars,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dimensionality of the output and book size was obtained: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the output and input layers\n",
    "K = len(book_chars)\n",
    "book_size = len(book_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow you to easily go between a character and its one-hot encoding and\n",
    "in the other direction a map dictionary was defined (one for every direction index to character and viceversa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mappings from char to ind and vice versa, mapped in a dictionary\n",
    "char_to_ind = { character:index for index, character in enumerate(book_chars)}\n",
    "ind_to_char = { index:character for index,character in enumerate(book_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these operations are included in a class named LoadBook that initizalize and store all these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import LoadBook\n",
    "\n",
    "# Initialize the class\n",
    "book = LoadBook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Set hyper-parameters & initialize the RNN’s parameters\n",
    "\n",
    "We have to set all the parameters of the RNN. As recommended, this will be done using the RNN as a class. This class is defined, using the parameters that will be defined as follows. The dimension of the hidden state of the RNN's architecture is set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden state dimension\n",
    "m = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the learning rate, and the length of the input sequence are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "eta = 0.1\n",
    "# Input sequence length\n",
    "seq_length = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias vectors, are initizalized to zero: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need numpy\n",
    "import numpy as np\n",
    "\n",
    "# bias vector of size m x 1 in equation for at\n",
    "b = np.zeros((m, 1))\n",
    "# bias vector of size C x 1 in equation for ot\n",
    "c = np.zeros((book.K, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the weight matrixes are randomly initialized as (take into account that input size is the same as output size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigma\n",
    "sig = 0.01\n",
    "\n",
    "# weight matrix of size m x d applied to xt (input-to-hidden connection)\n",
    "U = np.random.randn(m, book.K)*sig\n",
    "\n",
    "# weight matrix of size m x m applied to ht-1 (hidden-to-hidden connection)\n",
    "W = np.random.randn(m, m)*sig\n",
    "\n",
    "# weight matrix of size C x m applied to at (hidden-to-output connection)\n",
    "V = np.random.randn(book.K,m)*sig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recommended in the lab notes, all the parameters of the RNN will be stored in a class with the same name. We do then: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import RNN, LoadBook\n",
    "\n",
    "# Load the book\n",
    "book = LoadBook()\n",
    "\n",
    "# Initialize the network\n",
    "rnn = RNN(book.K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Synthesizing test from the randomly initizalized Weigths and Bias\n",
    "\n",
    "During training, and to check the continuous improvement of the performance, some example text will be Synthesize. Therefore, a function to Synthesize text must be written.  This function must take as input, the RNN, the hidden state vector h0 at time 0, another vector that represents the first input vector to the RNN (x0) and an integrer n denoting the lenght of the synthesized text.  The next input vector xnext is obtained from the current input vector x .  At each time step t when the vector of probabilities is generated, the label must be extracted from this probability distribution.   This sample will then be the t + 1th character in your sequence and will be the input vector for the next time-step of the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pKz \\xe22A7Dz/vprq)^WLC2)?^LIWzh\\xe2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# States initialization \n",
    "x_t = np.zeros((K,1))\n",
    "# First dummy charachter\n",
    "x_t[char_to_ind['d']] = 1\n",
    "\n",
    "# We don't want to modify the value of the hidden in the network, just use it\n",
    "hidden = rnn.hidden_init\n",
    "\n",
    "# Length of the output text\n",
    "n = 30\n",
    "\n",
    "# We need to store the text\n",
    "text = [] \n",
    "\n",
    "for t in range (n):\n",
    "    # Prediction\n",
    "    a_t = np.dot(rnn.W,hidden) + np.dot(rnn.U,x_t) + rnn.b\n",
    "    hidden = np.tanh(a_t)\n",
    "    o_t = np.dot(rnn.V,hidden) + rnn.c\n",
    "    e = np.exp(o_t)\n",
    "    p_t = e / e.sum()\n",
    "\n",
    "    # Randomly pick a character\n",
    "    cp = np.cumsum(p_t)\n",
    "    # Random number from normal distribution\n",
    "    a = np.random.uniform()\n",
    "    ind = np.array(np.nonzero(cp - a > 0))\n",
    "    ind = ind[0,0]\n",
    "    \n",
    "    # take sampled index as an input to next sampling\n",
    "    x_t = np.zeros((K,1))\n",
    "    x_t[ind] = 1\n",
    "    \n",
    "    # Save the computed character\n",
    "    text.append(ind)\n",
    "     \n",
    "# The final text is \n",
    "text_char = ''.join([ind_to_char[character] for character in text])\n",
    "text_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function operations are implemented in the Functions class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import RNN, LoadBook, Functions\n",
    "\n",
    "# Load the book\n",
    "book = LoadBook()\n",
    "\n",
    "# Initialize the network\n",
    "rnn = RNN(book.K)\n",
    "\n",
    "# Initialize functions\n",
    "functions = Functions(rnn,book)\n",
    "\n",
    "# Length of the output text\n",
    "n = 40\n",
    "\n",
    "# Get the randomly Synthesized text\n",
    "text = functions.Synthesize_text(char_to_ind['d'], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S,6K\\xc3}l:ZFr\"m,mZ\\tnhB/bqu\"b0D!oC^\\'-gB\\nt0v'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_char = ''.join([ind_to_char[character] for character in text])\n",
    "text_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Implement the forward & backward pass of back-prop\n",
    "\n",
    "Next up is writing the code to compute the gradients of the loss w.r.t. the parameters of the model.  The first seq_length characters of book data will be set as the labelled sequence for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_chars = book_data[0:seq_length]\n",
    "Y_chars = book_data[1:seq_length+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the label for an input character is the next character in the book. X_chars and Y_chars have to be converted to matrices X and Y containing the one-hot encoding of the characters of the sequence. Both X and Y have size K × seq_length and each column of the respective matrices corresponds to an input vector and its target output vector. This is done: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARRY POTTER AND THE GOBL\n"
     ]
    }
   ],
   "source": [
    "# Initialize the matrices\n",
    "X = np.zeros((book.K,rnn.seq_length))\n",
    "Y = np.zeros((book.K,rnn.seq_length))\n",
    "\n",
    "# One hot encoding\n",
    "for i in range(rnn.seq_length):\n",
    "    X[char_to_ind[X_chars[i]],i] = 1\n",
    "    Y[char_to_ind[Y_chars[i]],i] = 1\n",
    "\n",
    "dumb = []\n",
    "# We checked if it worked\n",
    "for i in range(rnn.seq_length):\n",
    "    for j in range(book.K): \n",
    "        if X[j,i] == 1:\n",
    "            dumb.append(ind_to_char[j])\n",
    "print(''.join([character for character in dumb ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the one hot encoding as a function of the class Functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import RNN, LoadBook, Functions\n",
    "\n",
    "# Load the book\n",
    "book = LoadBook()\n",
    "\n",
    "# Initialize the network\n",
    "rnn = RNN(book.K)\n",
    "\n",
    "# Initialize functions\n",
    "functions = Functions(rnn,book)\n",
    "\n",
    "# Test strings\n",
    "X_chars = book_data[0:seq_length]\n",
    "Y_chars = book_data[1:seq_length+1]\n",
    "\n",
    "# Onte hot encoding\n",
    "X,Y = functions.char_to_hot(X_chars, Y_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forward pass must be implemented. Following the instructions in the lab notes and reusing the code in the syntesize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2542.8033831179364"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test strings\n",
    "X_chars = book_data[0:seq_length]\n",
    "Y_chars = book_data[1:seq_length+1]\n",
    "\n",
    "# One hot encoding\n",
    "x_t,y_t = functions.char_to_hot(X_chars, Y_chars)\n",
    "\n",
    "\n",
    "# Initizalize hidden\n",
    "hidden = np.zeros((m,rnn.seq_length + 1))\n",
    "# We don't want to modify the value of the hidden in the network, just use it\n",
    "hidden[:,-1] = rnn.hidden_init[:,0]\n",
    "\n",
    "# Probabilities matrix\n",
    "p = np.zeros((book.K,seq_length))\n",
    "\n",
    "# Cost initialization\n",
    "cost = 0\n",
    "\n",
    "for t in range(rnn.seq_length): \n",
    "    \n",
    "    # find new hidden state\n",
    "    a_t = np.dot(rnn.W, hidden[:,t]) + np.dot(rnn.U, x_t[:,t]) + rnn.b.squeeze()\n",
    "    hidden[:,t + 1] = np.tanh(a_t)\n",
    "\n",
    "    # unnormalized log probabilities for next chars o_t\n",
    "    o_t = np.dot(rnn.V, hidden[:,t + 1]) + rnn.c.squeeze()\n",
    "    \n",
    "    e = np.exp(o_t)\n",
    "    # Softmax\n",
    "    if t == 0: \n",
    "        p[:,t] = e / e.sum()\n",
    "    else: \n",
    "        p[:,t] = e / e.sum()\n",
    "\n",
    "    # Cross-entropy loss\n",
    "cost = -np.log((p*y_t).sum(axis = 1) + np.finfo(float).eps).sum()\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is implemented as a function of the class functions. Let's check if it was correctly implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import RNN, LoadBook, Functions\n",
    "\n",
    "# Load the book\n",
    "book = LoadBook()\n",
    "\n",
    "# Initialize the network\n",
    "rnn = RNN(book.K)\n",
    "\n",
    "# Initialize functions\n",
    "functions = Functions(rnn,book)\n",
    "\n",
    "# Test strings\n",
    "X_chars = book.book_data[0:rnn.seq_length]\n",
    "Y_chars = book.book_data[1:rnn.seq_length+1]\n",
    "\n",
    "# Onte hot encoding\n",
    "X,Y = functions.char_to_hot(X_chars, Y_chars)\n",
    "\n",
    "# Forward - pass\n",
    "p, cost = functions.forward_pass(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the characters of the book are now in the vector book_data. Now we need a vector with the unique classes of characters in the book: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2542.8049462457648"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to implement the backward pass: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "functions.grads_to_zero()\n",
    "\n",
    "dh_next = np.zeros_like(hidden[:,0])\n",
    "\n",
    "for t in reversed(range(rnn.seq_length)):\n",
    "\n",
    "        # gradient w.r.t. o_t\n",
    "        g = p[:,t] - Y[:,t]\n",
    "        \n",
    "        # Bias c gradient update\n",
    "        functions.RNN.grad_c[:,0] += g\n",
    "        \n",
    "        # gradient w.r.t. V and c\n",
    "        functions.RNN.grad_V += np.outer(g, hidden[:,t+1])\n",
    "\n",
    "        # gradient w.r.t. h, tanh nonlinearity\n",
    "        dh = (1 - hidden[:,t+1] ** 2) * (np.dot(functions.RNN.V.T, g) + dh_next)\n",
    "\n",
    "        # gradient w.r.t. U\n",
    "        functions.RNN.grad_U += np.outer(dh, X[:,t])\n",
    "\n",
    "        # gradient w.r.t W\n",
    "        functions.RNN.grad_W += np.outer(dh, hidden[:,t])\n",
    "\n",
    "        # gradient w.r.t. b\n",
    "        functions.RNN.grad_b[:,0] += dh\n",
    "\n",
    "        # Next (previous) dh\n",
    "        dh_next = np.dot(functions.RNN.W.T, dh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is implemented as a function of the class functions. Let's check if it was correctly implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Lab4 import RNN, LoadBook, Functions\n",
    "\n",
    "# Load the book\n",
    "book = LoadBook()\n",
    "\n",
    "# Initialize the network\n",
    "rnn = RNN(book.K)\n",
    "\n",
    "# Initialize functions\n",
    "functions = Functions(rnn,book)\n",
    "\n",
    "# Test strings\n",
    "X_chars = book.book_data[0:rnn.seq_length]\n",
    "Y_chars = book.book_data[1:rnn.seq_length+1]\n",
    "\n",
    "# Onte hot encoding\n",
    "X,Y = functions.char_to_hot(X_chars, Y_chars)\n",
    "\n",
    "# Forward - pass\n",
    "p, cost = functions.forward_pass(X,Y)\n",
    "\n",
    "functions.backward_pass(X, Y, p, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010248988667696233"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
